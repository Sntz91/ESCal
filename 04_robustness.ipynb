{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428449ff-6286-4cbb-b506-7201c727f274",
   "metadata": {},
   "source": [
    "# How Robust are Homographies?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f58663-274a-4ed5-b215-79eab146d445",
   "metadata": {},
   "source": [
    "To test robustness, let's check how robust against\n",
    "- random noise\n",
    "- outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e081e0f-ef58-4433-8821-e2e326b88ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scienceplots\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import pprint\n",
    "from shapely.geometry import Polygon\n",
    "plt.style.use(['science', 'grid', 'ieee'])\n",
    "\n",
    "from utils import (\n",
    "    load_dataset,\n",
    "    get_images,\n",
    "    transform_points,\n",
    "    get_densities,\n",
    "    predict,\n",
    "    calculate_errors,\n",
    "    get_result_dict,\n",
    "    SCALING_FACTOR,\n",
    "    plot_setup, \n",
    "    plot_setup_noised,\n",
    "    plot_predictions,\n",
    "    add_outlier\n",
    ")\n",
    "\n",
    "def filter_points(points, keys):\n",
    "    filtered = {key: points[key] for key in keys}\n",
    "    return filtered\n",
    "\n",
    "def conduct_experiment(pv_img, tv_img, plot=True, print_errors=True, used_ref_points=None, return_homography=False):\n",
    "    # Load Data\n",
    "    reference_pts_pv, reference_pts_tv, validation_pts_pv, validation_pts_tv = load_dataset(pv_img, tv_img)\n",
    "    img_pv, img_tv = get_images(pv_img, tv_img)\n",
    "    \n",
    "    if used_ref_points:\n",
    "        reference_pts_pv = filter_points(reference_pts_pv, used_ref_points)\n",
    "        reference_pts_tv = filter_points(reference_pts_tv, used_ref_points)\n",
    "        \n",
    "    # Transform Points to Homogeneous Numpy arrays\n",
    "    reference_pts_pv_arr, reference_pts_tv_arr, validation_pts_pv_arr, validation_pts_tv_arr = transform_points(\n",
    "        reference_pts_pv, reference_pts_tv, validation_pts_pv, validation_pts_tv)\n",
    "    # Calculate Homography\n",
    "    h, _ = cv2.findHomography(\n",
    "        reference_pts_pv_arr,\n",
    "        reference_pts_tv_arr,\n",
    "        # method = cv2.RANSAC,\n",
    "        method = 0,\n",
    "    )\n",
    "    h_inv = np.linalg.inv(h)\n",
    "\n",
    "    # Get Pixel Densities\n",
    "    reference_densities = get_densities(reference_pts_tv_arr, h_inv)\n",
    "    validation_densities = get_densities(validation_pts_tv_arr, h_inv)\n",
    "    \n",
    "    # Predict Points\n",
    "    predicted_reference_pts_tv = predict(reference_pts_pv_arr, h)\n",
    "    predicted_validation_pts_tv = predict(validation_pts_pv_arr, h)\n",
    "    \n",
    "    # Errors\n",
    "    reference_errors = calculate_errors(predicted_reference_pts_tv, reference_pts_tv_arr)\n",
    "    validation_errors = calculate_errors(predicted_validation_pts_tv, validation_pts_tv_arr)\n",
    "\n",
    "    # Combine Results in Dictionary\n",
    "    reference_result_dict = get_result_dict(reference_pts_pv, reference_pts_tv_arr, predicted_reference_pts_tv, reference_errors, reference_densities, reference_pts_pv_arr)\n",
    "    validation_result_dict = get_result_dict(validation_pts_pv, validation_pts_tv_arr, predicted_validation_pts_tv, validation_errors, validation_densities, reference_pts_pv_arr)\n",
    "    \n",
    "    # Print Errors\n",
    "    if print_errors:\n",
    "        print(f'Reference  Error: {np.mean(reference_errors) :.2f} PX!')\n",
    "        print(f'Reference  Error: {np.mean(reference_errors) / SCALING_FACTOR * 100 :.2f} CM!')\n",
    "        print('---')\n",
    "        print(f'Validation Error: {np.mean(validation_errors):.2f} PX!')\n",
    "        print(f'Validation Error: {np.mean(validation_errors) / SCALING_FACTOR * 100 :.2f} CM!')\n",
    "    \n",
    "    # Plots\n",
    "    if plot:\n",
    "        plot_setup(img_tv, img_pv, reference_result_dict, validation_result_dict)\n",
    "        plot_predictions(img_tv, img_pv, reference_result_dict, validation_result_dict)\n",
    "    if return_homography:\n",
    "        return reference_result_dict, validation_result_dict, h\n",
    "    return reference_result_dict, validation_result_dict\n",
    "\n",
    "def get_collinearity_score(subset, reference_pts_pv):\n",
    "    A = np.array(reference_pts_pv[subset[0]])\n",
    "    B = np.array(reference_pts_pv[subset[1]])\n",
    "    C = np.array(reference_pts_pv[subset[2]])\n",
    "    AB = np.linalg.norm(A-B) \n",
    "    AC = np.linalg.norm(A-C) \n",
    "    BC = np.linalg.norm(B-C)\n",
    "    distances = sorted([AB, AC, BC])\n",
    "    return (distances[0] + distances[1] - distances[2]) / distances[2] * 100\n",
    "    \n",
    "def get_overall_collinearity_score(used_ref_pts, reference_pts_pv):\n",
    "    scores = []\n",
    "    for subset in combinations(used_ref_pts, 3):\n",
    "        score = get_collinearity_score(subset, reference_pts_pv)\n",
    "        scores.append(score)\n",
    "    return min(scores)\n",
    "\n",
    "def get_collinearity_scores(used_ref_pts, reference_pts_pv):\n",
    "    scores = []\n",
    "    for subset in combinations(used_ref_pts, 3):\n",
    "        score = get_collinearity_score(subset, reference_pts_pv)\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0d69ce-22bb-4593-8da2-d828ee6b2f01",
   "metadata": {},
   "source": [
    "## Random Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b771407-1794-4d88-b5a9-4bb0b3e670cd",
   "metadata": {},
   "source": [
    "Let's take our reference points and add random noise (gaussian) around it and conduct the experiments. We can make the same experiments as in the overall case, with all k and validation points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47395d23-cd32-46dc-8839-43b38d84c55b",
   "metadata": {},
   "source": [
    "We can conduct like 10 times random and one time the normal and then see how the result differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c40a31-e587-444a-8ca4-3f1895e745b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(points, m=0, std=1):\n",
    "    zeros = np.zeros((points.shape[0], 1))\n",
    "    noise = np.random.normal(m, std, points[:,:-1].shape)\n",
    "    noise = np.hstack((noise, zeros))\n",
    "    return points + noise\n",
    "\n",
    "\n",
    "def get_result_dict_noised(\n",
    "    point_dict_pv, \n",
    "    point_arr_tv, \n",
    "    predicted_points_tv, \n",
    "    errors, \n",
    "    densities, \n",
    "    point_arr_pv_noised, \n",
    "    point_arr_tv_noised\n",
    "):\n",
    "    error_dict = {}\n",
    "    for i, (name, coord) in enumerate(point_dict_pv.items()):\n",
    "        error_dict[name] = {}\n",
    "        error_dict[name]['coordinates_pv'] = coord\n",
    "        error_dict[name]['coordinates_tv'] = (point_arr_tv[i][0], point_arr_tv[i][1])\n",
    "        error_dict[name]['coordinates_pv_noised'] = (point_arr_pv_noised[i][0], point_arr_pv_noised[i][1])\n",
    "        error_dict[name]['coordinates_tv_noised'] = (point_arr_tv_noised[i][0], point_arr_tv_noised[i][1])\n",
    "        error_dict[name]['predicted_coordinates_tv'] = (predicted_points_tv[i][0], predicted_points_tv[i][1])\n",
    "        error_dict[name]['error'] = errors[i]\n",
    "        error_dict[name]['pixel_density'] = densities[i]\n",
    "    return error_dict\n",
    "\n",
    "def conduct_experiment_noised(pv_img, tv_img, basic_homography, plot=True, print_errors=True, used_ref_points=None, n_mean=0, n_std=100):\n",
    "    # Load Data\n",
    "    reference_pts_pv, reference_pts_tv, validation_pts_pv, validation_pts_tv = load_dataset(pv_img, tv_img)\n",
    "    img_pv, img_tv = get_images(pv_img, tv_img)\n",
    "    \n",
    "    if used_ref_points:\n",
    "        reference_pts_pv = filter_points(reference_pts_pv, used_ref_points)\n",
    "        reference_pts_tv = filter_points(reference_pts_tv, used_ref_points)\n",
    "        \n",
    "    # Transform Points to Homogeneous Numpy arrays\n",
    "    reference_pts_pv_arr, reference_pts_tv_arr, validation_pts_pv_arr, validation_pts_tv_arr = transform_points(\n",
    "        reference_pts_pv, reference_pts_tv, validation_pts_pv, validation_pts_tv)\n",
    "\n",
    "    # Add Noise\n",
    "    reference_pts_pv_arr_noised = add_noise(reference_pts_pv_arr, n_mean, n_std)\n",
    "    reference_pts_tv_arr_noised = add_noise(reference_pts_tv_arr, n_mean, n_std)\n",
    "    \n",
    "    # Calculate Homography\n",
    "    h, _ = cv2.findHomography(\n",
    "        reference_pts_pv_arr_noised,\n",
    "        reference_pts_tv_arr_noised,\n",
    "        # method = cv2.RANSAC,\n",
    "        method = 0,\n",
    "    )\n",
    "    h_inv = np.linalg.inv(h)\n",
    "\n",
    "    # Get Pixel Densities\n",
    "    reference_densities = get_densities(reference_pts_tv_arr, h_inv)\n",
    "    # The densities should be calculated \"perfectly\" for the validation point, \n",
    "    # hence we should use a good homography for this, \n",
    "    # hence, we choose the baseline homography\n",
    "    validation_densities = get_densities(validation_pts_tv_arr, np.linalg.inv(basic_homography)) # important trick!\n",
    "    \n",
    "    # Predict Points\n",
    "    predicted_reference_pts_tv = predict(reference_pts_pv_arr_noised, h)\n",
    "    predicted_validation_pts_tv = predict(validation_pts_pv_arr, h)\n",
    "    \n",
    "    # Errors\n",
    "    reference_errors = calculate_errors(predicted_reference_pts_tv, reference_pts_tv_arr)\n",
    "    validation_errors = calculate_errors(predicted_validation_pts_tv, validation_pts_tv_arr)\n",
    "\n",
    "    # Combine Results in Dictionary\n",
    "    reference_result_dict = get_result_dict_noised(\n",
    "        reference_pts_pv, \n",
    "        reference_pts_tv_arr, \n",
    "        predicted_reference_pts_tv, \n",
    "        reference_errors, \n",
    "        reference_densities, \n",
    "        reference_pts_pv_arr_noised, \n",
    "        reference_pts_tv_arr_noised\n",
    "    )\n",
    "    validation_result_dict = get_result_dict(validation_pts_pv, validation_pts_tv_arr, predicted_validation_pts_tv, validation_errors, validation_densities, reference_pts_pv_arr)\n",
    "     \n",
    "    # Print Errors\n",
    "    if print_errors:\n",
    "        print(f'Reference  Error: {np.mean(reference_errors) :.2f} PX!')\n",
    "        print(f'Reference  Error: {np.mean(reference_errors) / SCALING_FACTOR * 100 :.2f} CM!')\n",
    "        print('---')\n",
    "        print(f'Validation Error: {np.mean(validation_errors):.2f} PX!')\n",
    "        print(f'Validation Error: {np.mean(validation_errors) / SCALING_FACTOR * 100 :.2f} CM!')\n",
    "     \n",
    "    # Plots\n",
    "    if plot:\n",
    "        plot_setup_noised(img_tv, img_pv, reference_result_dict, validation_result_dict)\n",
    "        # plt.savefig('random_noise.png', dpi=300)\n",
    "        plot_predictions(img_tv, img_pv, reference_result_dict, validation_result_dict)\n",
    "    return reference_result_dict, validation_result_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a8b3d-2485-43eb-a079-9f3c825b3d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_val, baseline_ref, baseline_h = conduct_experiment('IMG_01', 'IMG_00', plot=False, return_homography=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456b547-77c4-46c9-ba59-080c35b455cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = conduct_experiment_noised('IMG_01', 'IMG_00', baseline_h, plot=False, n_std=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba7323-b5e3-497a-bde1-6a2fb0167c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def noised_experiment(pv_img, tv_img, n_mean, n_stds, baseline_h):\n",
    "    reference_pts_pv, reference_pts_tv, validation_pts_pv, validation_pts_tv = load_dataset(pv_img, tv_img)\n",
    "    rows = []\n",
    "\n",
    "    for n_std in n_stds:\n",
    "        print('-'*20, n_mean, n_std,'-'*20)\n",
    "        for i in range(100):\n",
    "            print('#'*10)\n",
    "            print(f'Experiment {i}')\n",
    "            reference_result_dict, validation_result_dict = conduct_experiment_noised(pv_img, 'IMG_00', baseline_h, plot=False, n_mean=n_mean, n_std=n_std)  \n",
    "    \n",
    "            for pt, pt_dict in validation_result_dict.items():\n",
    "                pt_dict['experiment'] = i\n",
    "                pt_dict['name'] = pt\n",
    "                pt_dict['n_mean'] = n_mean\n",
    "                pt_dict['n_std'] = n_std\n",
    "                rows.append(pt_dict)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['img'] = pv_img\n",
    "    return df\n",
    "noise_results_0026 = noised_experiment('IMG_01', 'IMG_00', 0, [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70], baseline_h)\n",
    "# noise_results_0029 = noised_experiment('IMG_02', 'IMG_00', 0, [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70], baseline_h)\n",
    "# noise_results_0045 = noised_experiment('IMG_06', 'IMG_00', 0, [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70], baseline_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912a3b5-9bf6-4e95-98dc-990c94c157fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ADD 0!!!\n",
    "fig, ax = plt.subplots(1, 1)#, figsize=(10, 10))\n",
    "\n",
    "grouped = noise_results_0026.groupby('n_std').agg(\n",
    "    error_mean=('error', lambda x: np.mean(x).round(2)),\n",
    "    error_std=('error', lambda x: np.std(x).round(2))\n",
    ").reset_index()\n",
    "\n",
    "means = grouped['error_mean'].values\n",
    "stds = grouped['error_std'].values\n",
    "noise = grouped['n_std'].values\n",
    "\n",
    "means = np.insert(means, 0, 3.8, axis=0)\n",
    "stds = np.insert(stds, 0, 0, axis=0)\n",
    "noise = np.insert(noise, 0, 0, axis=0)\n",
    "\n",
    "ax.plot(noise, means, c='black', marker='o')\n",
    "ax.fill_between(noise, means-stds,means+stds,alpha=.1, color='black')\n",
    "ax.set_xlabel('noise [px]')\n",
    "ax.set_ylabel('error [px]')\n",
    "ax.grid(False)\n",
    "\n",
    "plt.savefig('output/noise_results.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d023d-fd55-45c4-82c2-09b5b5d6c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_result_dict, validation_result_dict = conduct_experiment('IMG_06', 'IMG_00', plot=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1d57d-6bf9-4e07-9aad-1ec28c7ae47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ADD 0!!!\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "grouped = noise_results_0045.groupby('n_std').agg(\n",
    "    error_mean=('error', lambda x: np.mean(x).round(2)),\n",
    "    error_std=('error', lambda x: np.std(x).round(2))\n",
    ").reset_index()\n",
    "\n",
    "means = grouped['error_mean'].values\n",
    "stds = grouped['error_std'].values\n",
    "noise = grouped['n_std'].values\n",
    "\n",
    "# Add zero experiment\n",
    "means = np.insert(means, 0, 34.93, axis=0)\n",
    "stds = np.insert(stds, 0, 0, axis=0)\n",
    "noise = np.insert(noise, 0, 0, axis=0)\n",
    "\n",
    "ax.plot(noise, means, c='steelblue', marker='o')\n",
    "ax.fill_between(noise, means-stds,means+stds,alpha=.1, color='steelblue')\n",
    "ax.set_xlabel('noise [px]')\n",
    "ax.set_ylabel('error [px]')\n",
    "\n",
    "plt.savefig('output/noise_results_06.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085116d-6cc4-450b-89ad-f5d9a8c5400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ADD 0!!!\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "grouped = noise_results_0029.groupby('n_std').agg(\n",
    "    error_mean=('error', lambda x: np.mean(x).round(2)),\n",
    "    error_std=('error', lambda x: np.std(x).round(2))\n",
    ").reset_index()\n",
    "\n",
    "means = grouped['error_mean'].values\n",
    "stds = grouped['error_std'].values\n",
    "noise = grouped['n_std'].values\n",
    "\n",
    "# Add zero experiment\n",
    "means = np.insert(means, 0, 3.83, axis=0)\n",
    "stds = np.insert(stds, 0, 0, axis=0)\n",
    "noise = np.insert(noise, 0, 0, axis=0)\n",
    "\n",
    "ax.plot(noise, means, c='steelblue', marker='o')\n",
    "ax.fill_between(noise, means-stds,means+stds,alpha=.1, color='steelblue')\n",
    "ax.set_xlabel('noise [px]')\n",
    "ax.set_ylabel('error [px]')\n",
    "\n",
    "plt.savefig('output/noise_results_02.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f629e2-b883-4ee8-ba24-093adb5d8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_results = noise_results_0026\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "noise_results.boxplot(\n",
    "    column='error',\n",
    "    by='n_std',\n",
    "    showfliers=True,\n",
    "    vert=False,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightgray', lw=1),\n",
    "    medianprops=dict(color='black', lw=2),\n",
    "    whiskerprops = dict(color = \"black\", lw=1),\n",
    "    widths=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('')\n",
    "fig.suptitle('')\n",
    "ax.set_ylabel('standard deviation')\n",
    "ax.set_xlabel('error')\n",
    "ax.axvline(6.95, lw=3, ls='dashed', color='black')\n",
    "plt.savefig('output/noised_experiment_results.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdaba69-6252-497c-9f9e-d742916b4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW CAN THE PIXEL DENSITY BE BIGGER IN CASE n_STD = 50?? THEY SHOULD BE ALWAYS THE SAME ... \n",
    "# Pixel densities will be slightly off, depending on how far you move the reference point,\n",
    "# your predicted validation points into the perspective view will be off!\n",
    "# You should use a \"perfect\" Homography for this; so the 6.95 basic homography!\n",
    "\n",
    "\n",
    "# noise_results.loc[noise_results['n_std'] == 10].plot.scatter(\n",
    "noise_results.loc[noise_results['n_std'] == 10].plot.scatter(\n",
    "    x='pixel_density',\n",
    "    y='error',\n",
    "    # c='n_std',\n",
    "    s= 200, \n",
    "    # colormap='viridis',\n",
    "    # alpha=0.4,\n",
    "    figsize=(15, 10)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe331a-487b-418e-a1dd-3600a4e69066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I Think some kind of barchart should be better\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "noise_results.loc[noise_results['n_std'] == 50].boxplot(\n",
    "    column='error',\n",
    "    by='pixel_density',\n",
    "    showfliers=True,\n",
    "    vert=False,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightgray', lw=1),\n",
    "    medianprops=dict(color='black', lw=2),\n",
    "    whiskerprops = dict(color = \"black\", lw=1),\n",
    "    widths=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "# ax.axvline(6.95, lw=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a1032-01a6-4b01-9423-ce3153838051",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "noise_results.loc[noise_results['n_std'] == 5].boxplot(\n",
    "    column='error',\n",
    "    by='pixel_density',\n",
    "    showfliers=True,\n",
    "    vert=False,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightgray', lw=1),\n",
    "    medianprops=dict(color='black', lw=2),\n",
    "    whiskerprops = dict(color = \"black\", lw=1),\n",
    "    widths=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "# ax.axvline(6.95, lw=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63fbe0-07ae-4fbe-baf9-77dd54b0a113",
   "metadata": {},
   "source": [
    "## 2. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed0096-32e6-4e45-b0a4-37edd83b6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment_outlier(pv_img, tv_img, basic_homography, plot=True, print_errors=True, used_ref_points=None, n_mean=0, n_std=100):\n",
    "    # Load Data\n",
    "    reference_pts_pv, reference_pts_tv, validation_pts_pv, validation_pts_tv = load_dataset(pv_img, tv_img)\n",
    "    img_pv, img_tv = get_images(pv_img, tv_img)\n",
    "    \n",
    "    if used_ref_points:\n",
    "        reference_pts_pv = filter_points(reference_pts_pv, used_ref_points)\n",
    "        reference_pts_tv = filter_points(reference_pts_tv, used_ref_points)\n",
    "        \n",
    "    # Transform Points to Homogeneous Numpy arrays\n",
    "    reference_pts_pv_arr, reference_pts_tv_arr, validation_pts_pv_arr, validation_pts_tv_arr = transform_points(\n",
    "        reference_pts_pv, reference_pts_tv, validation_pts_pv, validation_pts_tv)\n",
    "\n",
    "    # Add Noise\n",
    "    reference_pts_pv_arr_noised = add_outlier(reference_pts_pv_arr, 2, 50)\n",
    "    reference_pts_tv_arr_noised = add_outlier(reference_pts_tv_arr, 2, 50)\n",
    "    reference_pts_pv_arr_noised = add_outlier(reference_pts_pv_arr_noised, 3, 50)\n",
    "    reference_pts_tv_arr_noised = add_outlier(reference_pts_tv_arr_noised, 3, 50)\n",
    "    \n",
    "    # Calculate Homography\n",
    "    h_r, _ = cv2.findHomography(\n",
    "        reference_pts_pv_arr_noised,\n",
    "        reference_pts_tv_arr_noised,\n",
    "        method = cv2.RANSAC,\n",
    "        # method = 0,\n",
    "    )\n",
    "    h_r_inv = np.linalg.inv(h_r)\n",
    "    \n",
    "    h_ls, _ = cv2.findHomography(\n",
    "        reference_pts_pv_arr_noised,\n",
    "        reference_pts_tv_arr_noised,\n",
    "        # method = cv2.RANSAC,\n",
    "        method = 0,\n",
    "    )\n",
    "    h_ls_inv = np.linalg.inv(h_ls)\n",
    "\n",
    "    reference_results = []\n",
    "    validation_results = []\n",
    "    names = ['ls', 'r']\n",
    "    i = 0\n",
    "    for h, h_inv in ((h_ls, h_ls_inv), (h_r, h_r_inv)):\n",
    "        # Get Pixel Densities\n",
    "        reference_densities = get_densities(reference_pts_tv_arr, h_inv)\n",
    "        # The densities should be calculated \"perfectly\" for the validation point, \n",
    "        # hence we should use a good homography for this, \n",
    "        # hence, we choose the baseline homography\n",
    "        validation_densities = get_densities(validation_pts_tv_arr, np.linalg.inv(basic_homography)) # important trick!\n",
    "        \n",
    "        # Predict Points\n",
    "        predicted_reference_pts_tv = predict(reference_pts_pv_arr_noised, h)\n",
    "        predicted_validation_pts_tv = predict(validation_pts_pv_arr, h)\n",
    "        \n",
    "        # Errors\n",
    "        reference_errors = calculate_errors(predicted_reference_pts_tv, reference_pts_tv_arr)\n",
    "        validation_errors = calculate_errors(predicted_validation_pts_tv, validation_pts_tv_arr)\n",
    "    \n",
    "        # Combine Results in Dictionary\n",
    "        reference_result_dict = get_result_dict_noised(\n",
    "            reference_pts_pv, \n",
    "            reference_pts_tv_arr, \n",
    "            predicted_reference_pts_tv, \n",
    "            reference_errors, \n",
    "            reference_densities, \n",
    "            reference_pts_pv_arr_noised, \n",
    "            reference_pts_tv_arr_noised\n",
    "        )\n",
    "        validation_result_dict = get_result_dict(validation_pts_pv, validation_pts_tv_arr, predicted_validation_pts_tv, validation_errors, validation_densities, reference_pts_pv_arr)\n",
    "         \n",
    "        # Print Errors\n",
    "        if print_errors:\n",
    "            print(f'Reference  Error: {np.mean(reference_errors) :.2f} PX!')\n",
    "            print(f'Reference  Error: {np.mean(reference_errors) / SCALING_FACTOR * 100 :.2f} CM!')\n",
    "            print('---')\n",
    "            print(f'Validation Error: {np.mean(validation_errors):.2f} PX!')\n",
    "            print(f'Validation Error: {np.mean(validation_errors) / SCALING_FACTOR * 100 :.2f} CM!')\n",
    "         \n",
    "        # Plots\n",
    "        if plot:\n",
    "            plot_setup_noised(img_tv, img_pv, reference_result_dict, validation_result_dict)\n",
    "            plot_predictions(img_tv, img_pv, reference_result_dict, validation_result_dict, names[i])\n",
    "            i+=1\n",
    "        reference_results.append(reference_result_dict), validation_results.append(validation_result_dict)\n",
    "    plt.show()\n",
    "    return reference_results, validation_results\n",
    "baseline_val, baseline_ref, baseline_h = conduct_experiment('IMG_01', 'IMG_00', plot=False, return_homography=True)\n",
    "reference_results, validation_results = conduct_experiment_outlier('IMG_01', 'IMG_00', baseline_h, plot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137ecba-0648-4f86-83ee-37c879dc1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = conduct_experiment('IMG_01', 'IMG_00', plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
